{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ThemeRegistry.enable('dark')"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Libraries for this section \n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import altair as alt\n",
    "from IPython.display import YouTubeVideo\n",
    "\n",
    "alt.themes.enable('dark')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Vectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/jpeg": "/9j/4AAQSkZJRgABAQAAAQABAAD/2wCEAAUDBAgICAgICAgICAgGBwgIBwcHBwgICAgICAgICAgICAgIChALCAgPCggIDBUNDhERExMTCAsWGBcSGBASExIBBQUFBwYHDwgIDx4SERUeHR4eGxgdHR4XHRofHR4XHx4WFRUXFRcaFR4XHh4bFxcVHRgXFhgWGxoXHx0eGB4VF//AABEIAWgB4AMBIgACEQEDEQH/xAAdAAEAAgMAAwEAAAAAAAAAAAAABwgEBQYCAwkB/8QAThAAAQQBAwIEAgMIDA0FAQAAAAECAwQFBhESEyEHCDFBFCIyUbQVNDZhc3SBswkWIyRCQ1JUVXWRlBgzN2JxgpOVodHS09Q1U2S11SX/xAAbAQEAAgMBAQAAAAAAAAAAAAAAAgUDBAYBB//EADMRAQABAgQEAgcIAwAAAAAAAAABAgMEBREhEjFRsUGBEzM0coKRoQYjJTJhccHxB2LR/9oADAMBAAIRAxEAPwCmQAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAPY+B7Wo5WPRrk3a5WqjVT60VU2VDxYxXLs1FVV9ERFVf7EJd8LPMTqXBLj60d7rYrHPYx2MkrVFY+rzV0kLZul1mOVHO2dz3RdvVOxdjzNawytTSD83pm1Gxzfgra22V47SvxllWostdsrXRp/joHq9zXIkaSL2XZyB8y5Y3MVWva5rk23a5Faqbpum6L39FRf0ngbfWGpLuXuz5HIzrZu3FYs9hzI2K9YomQs3ZE1rE2jjY3sifRNQB7HQvRqPVjkY76L1aqNXZVRdnei90VP0CKB70crWOcjE3erWqqNRO6q5UTsm31lmPJF4q52LL47TqSsmwaRX3zV5YIkSjE2OzdktNtMaj40669+o5zV6ytREVzVTQeOPmdzubmyFOjOyjg7TZ6jKrK0LprVR6Oj52Z5mOkZI9iqqtjViNR+3dU5KEBAAAAAAAAAHWaa0at3C53MJYSNunH4pi1ulyWyuUsTQJtJzTpcOirvou3327AcmAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAH0I8l2o4NR6Nnwd5UmdimTYm3Erk5vx1uORarlRPoN6bpoG/mh89ye/Ivrj7k6phqSv41dRxrj5Ec7ZqWlXqUX7fwpFlRYET/wCWoEO6309NiclexlhF62MuT1ZHKxWI/oyOY2VrXd+m9qI9q+7XtXvuaYtL+yJaI+EzNPNxM2izlboWnNauyXaSNYjnu9EV9Z0LUT3+FkXv7Vgp1pJpI4YmOklnkZHFGxN3Pke5GsY1E9XK5URE/GBM3hun3D0XqDOuTjc1RI3TGJVWqjkrPT4jMTN7/NG6JrYkcn0Xxe++xCZM3mlnjpT4fStdzXQaJxUdaw9ifLLlryMuZSdndflc9Yk4r3RzHpupwHhZou1qHL0sRU2bLfm4umc3kyCFjVknsPTdN2sja9226bqiIndUA0eIxlm5K2CpXntTv34QVYZJ5XbevGOJquX9CG11BofNY+PrX8PlKMO6J1r2Nt1o93Lsic5o2t3VVRPX3LF+ZPVEWiI62j9JOXHvSrHYz2WhVEyluSVF6MMlxu0kTlbvM7hx2SaJrODUc10T+D3jZmcJkoJp79u9jZpEjyuNuzyW4LNSVUbY2inerW2EYqq16bLuiIu7Vc1wRYCwvnY8Jamn8jVyWKjSLF6hbLI2vEi9Grbj4Okjh7bR15GStkYzfttKjdmtaibPyl+DuPs0rur9SRo/C4Zk81am9rnMtupMdLZsTxp/jq0fDg2JN0kej0cmzFa8IG0/orM5GPq4/EZO9Eiqiy0cdatRoqdlTnBG5N0NbmcTapSrBcrWKk7URXQW4JIJURd9lWOVqOROy+3sSJ4neOuoM1bfLHes4yjG7jQxWMsyVa1Su3tFGqV1Z15EaibyPT1V3FGN2ak6eWHR+S1xgMg3UOV+6GKjsTUqUV6J1zKUrzK8EzbtPJSSpLVRvXhTpO6jJGte1Wtau7gp9XhfI9scbXPklc1kcbGq573uVGta1re7nKqoiInruW0zfhpY054T5P46NYshmshjrtmBybSV4/i6sdatJ9UjWI56ovdrp3NX6JXvQfiDm9Lz20xFyKrLM9sdiVKdG5yWsszGrFJcgkWNv7rJ3Zx5I5N99k2u5r7xIzVXwyp6hgu8MvPTxUsl34Wo7k+zYhZMvw74VgTk17k2RnbftsB87wSvqHzF6yyFSzRt5jrVb9eWtah+5mKj6kMzFZIznFUa9m7XKm7VRU9lQjjTWGs5G5VoVI1ltZCxFXrxp25SSvRjd1Xs1u67q5eyIiqvZAMOvC+R7Y42ue+RzWRxsarnve5Ua1rWt7ucqqiIieu50l7w61DBCtifBZmGBreTrE2Juxwo1E3VyyPiRqN299ywnjFNW8M6NHCYFIv2yZWmtnMajfE11yGs96xpDQWRqpWjlkimaiNXdja7VXk97ZGwDi/E/UdayluHO5ZthHo9ZX5GzLzVF32mZK9zZmqvq16ORfdFA5E/WoqqiIm6r2RE9VX6kLU5jR1XxB0rNqfH1IqeqMG6WLNVqcSMgy/RjbO6VsDPo23xPSRr2pu57ZY1RydNzIB8M/EXMabsS2sNbSpNZiSGdy1q1hJIke2Tpq2zE9Gpyanduy9vUDS2cBfihWzJStx10VqLYkqzMhRXLs1Flc3iiqvp37n5j8DesQy2K9K3PXr79exBVmlhi4tRzurKxqtj2aqL3VOyn0G1bcZqfw9x+SzWzazoa2VzaVv3NZYMd1Z54YEVyKySd8DYW7L8q2EVPQgjy8+YuGtnbFjUE0tTFOoOrYihQZN9y8SjZWOjiixtZOK7xI6Pr8HSdtlVUe5UCr5u9OaRy2SRzsdjMjfbG7i91ChZtIx2yLxcsEbuLtlRdl+tCY9G4HDa08SJG1YHQYO5cnvSV9ug6aGtB1JG8Gd4m2LDN1RFa5rJ3bK1UTbnPH/X+bfnMlj/AImzjaGHu2KGPw1GSSlRqVKs0kVdrKsCtjV6xo1yybfNy7bN4ogR7ltKZSnPFVt42/VtWnNZXq2qNiCeZ7nIxrYoZWI+Ryuc1qI1F7uRPc1duvJDI+KVj4pYXujlilY5kkcjHK17HsciKx6ORUVF7oqKTp5VPFvJU9RYmjeszZDHZG/BVStfe60lOzY51q1yi6dVWnM19ji5zNuUckiKi/KrYx8Y/wAI9Qf1/lft84HKAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAB78fbkryxTwvdHNWlZNDKxdnRyRuR8b2r7ORzUX9B6AB9GfF6rHrrw5S/XY11l1CLL1WRosnTvUmvS7Vj27vfs27WT8bk7exUPyrYqH7sTZy4zlj9FULGbsoqdpZ67VTH12OVdmzusrG9u/r0FT8aT/APscet+rUyenpnorqL0yNFrnbuWvOrYbcbW7fLGyZIH/AI1uO9PfgvMnpyDReHvYKq6NZda6hnyL0iaidDA497X42jLu5VVUszK9rkTZejJ2T3Ct2fyk163au2Hc7GQtT2rD0TZHTWJXSyu29t3vcpZv9jdx0b85lrTu8lXENiiRduyWLUSyOT8adBrd/qev1lViZvJz4hQae1PBLckbDRysEmOuTyP4xwJM+OSCxJv8qMbPDE1z12RrJJF37LuHXeZxNIS6szL8nb1My98RCywyjjcXLVb0qleKNIZJ77JHt6bGd3NT37Eb/DaB/nur/wDdOF//AEjt/PtpCahquXIK1fhdRV4bNeTb5OtWhhq2oUX3eixxSr+K0wr4BYzzI+OOF1FpzE4XHwZT4jD2aj328lXqQpLFXoz1Hr+97UqpK98kb9tkT5V7+hN/iHGmP8GYUqokSS6ewTn8EREVclax8lxV293ran3X65FKc5nRLaencfmLMz4rebyM8ePx7o02mxlWJEmyCv35MT4lzYmtVPmRHOTshbnwVycetfDW9p5ro3ZTFY5cf8O5Ubu6svxGGl2VVVsLuhBGr1/hV5V22RAKInQaY1tmMXFYgx2TvUYbybWYqlqWFkvbju5rHInPj8vJO+yqm+ymjswPie+KRjo5InuZJHI1WPY9iq17Hscm7XIqKiovdFQ3vhtpaXN5bH4qFVa7I2mRPkREVIIE3ks2XIqonCKBksq/iiUDni8fin/kZx/9XYP7XWKWakr1ortuKnM6xTht2I6dmRqMfPWZK9sEz2J2a50aNcqe3Iuh4hzMseC1OSFyPZDRxDHuaqKjXw5OvWlb292zIrFT60UCj5YDyC4qKxrGGSRN3Y3G3rUG69uqrY6m+3vsy1Ivf32X2Q5Tylon7ccO923TgW9PKrtuLYoMbclkc7fsiI1imF5adds07qbG5Gdytp831b6tTfaraYsT5FREVytje6ObZvdejt77Adp595nO1nZa70hx9BjPxNWHqLt/rSO/tIDLP/sh2m5GZuhmokSSjmcZFEyzGvONbNZz1VObfl4ury13NXf5tpNvoqVgAuf+xmTOVmp41cvBjsM9rfZHPTKI9yJ9apGxP9VCq/izjYqeoM5TgajIKOcydaBjURGsiguzxRtaidkRGtRNvxFsPJY6HTGj87qnJosNe5OjoEcrWvtw49j4oGwcl2V8luxYgai7bub9Xcptn8pLet2rs6os+QtT2p1aio1ZbErpZFRFVVROT19wLpawnczwTr8VVOpRxzFVFVF4uzMHJO3sqJsv1oqlHy7mt/8AIpU/NMZ/9xCUjA6fwr1pZ09l6WYqI18uPlVyxPVUZNDIx0U8L1T0R8T3t5bLsqoqd0QulPrHwt1yyOxl/g6eQSNnVXJSyYq3GqIrWsdfjeyG2xNl2RZHoicd2tVUQrX5O9N1MvqRcfdhimht4fKRKksbZOm6SssaTRo5PllbzVUcmyovoqEV6lw1jHXLVC2xY7OPsS1rEa79pInqx2yr9Jqqm6L7oqL7gXL1j5Q8bPB90NI5maOzE5J6jZrUVmq+SPZzG17tZrZKz0cjXJIqybKiem+6Uy1HUtwXLUN9srbsFmaO62w5XTJZbI5Juq5VVXSc+W67ruvfud/5YtZZHEamxKUZntZlMnRo3qqOXpW4LNhkCskj32dI1JnOY5e7XbL9aLu/O2+uut8v0Nt0bRSwrduK2EoVue23bfjwRf8AOR2/fcCFgAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAd54Ba+XTWoMfllSR9eCR0V6KJU5y052rFOjWqqI97UckrWqqIr4md09UzvMp4jt1RqG1k4klbTZHFUx0c7WtkZVgaq/M1qqiK+Z88u267dbb2I1AAHT6d0Dlr9G7k69OT7n4uB81m7InTg2j+kyJ7v8AHyp7tZvtum+26HMGK3iLV2qqmiqJmnadJ5Tz0npOng9mJhMukPHaX7lMwGpcbDqTDQ8fhGWZ31shR4JxjWpfYjnI1jVe1qKnLi7hzRnymCmodC03/E08BmsjNyV0dTOZeszHxO7uY5zMfWbPaY13FOm96I5N91+uKAZXjf691fezd116/I10ixshgghYkVWnVhTjBTp12/LXqxt7NYn1uVVVznOXz8O9a5LT9+LJYuw6vZhRWrt80U0LlRZK9iJflmhdxaqtX0VrXJs5rVTnQBOWrvE3SWpH/GZ7T16jlXo1bd/TGQgijvPRuyvlqX4XsiVVX13e9dk3euxy+Y8RaNWlZxumsW7FRZKFa2Syt238dmL9RV+aqs7Y44qNWTZOpFAxOpwajnK1FasagATN4K+O0uExtvAZOgzM4DJtlZLSfYfXnrpYRGzrVnai8WKnJ/DZqo/ZzXRqrlWGQBKmT11gcdUv19L47IwWM5Wkp3cnm7kFmevj5lT4mjQgqxMjjbKjWMdNJzfw5tTblySKwAJa0B43WaeMdgczQg1FgXInTx16V8E9RW78Vo340dJW25Lt2dx/g8d3b/tbPeH9aT4qPA6hvSIvJuMyOaqRY9FVyKjVmqVUsvY30RFXunrvvuRIAJD8YPF3KalWvDOkFLGY1EZjcLjo+hRqMYzpRbRov7pK2L5EcvZqK5GIxHK00vhznMRRmmky+DTOxSQoyCu7KW8akEvNrut1KiK6X5Uc3ivb5t/Y5YAWkt+a+hLikwkmjKb8S2COumPdlnrCkUTmuiRP3nyRzXMa5H78uTUdvv3IM8SdQ4S/8N9x9PNwPR63xXHL3cklvqdLpdrafuHT4S/Q+l1u/wBFDjgBIXl/8RW6WzH3WdWdbWKjchiga9GIs00SthV7l9IuaN5bd9lXbdexusn4m4nPu6urcXYnyPJeWe0/Yr4+7PEir0YLlSaB9W0sbVSNsqIx6RwxNVXcdyIwBMWB8SNP6ekda01hbkuURrkq5fUt2C0tBXt4PfVx9KCOF0vFXcZJHO4qvoqKrVibKXprU81mxI+axbmknsTSLyfLNM9ZJZHu93ue5zlX61MYAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAZuDxNm9Yjq04JbNmw7jFBBG6SR6+q7Nam+yJuqr6IiKq9kLZ+CHlWji6d3UytmlTi+PDwSbwxrtvtdnYv7s5F7dONeHy93PRVQhDwH8ZLWlJZenSqXK9tyfEtfG2G2rU9GxXmNV7WoqIvB6Pb67Iiqql5vCLxIx+pqTrtBs7GwydGxFZhVjopuDXrHzTeOX5XNXdjl7ObvtvsfK/wDIGc57gqJpw9HBZnb0lM61T+nhNH8+E+Dewlu1VO+89Hp8ZMfDHpTO14Y44YYMBkelDDG2OKNkNOV7WMjYiNY1OKIiImx8zj6pa2qpPjMjAqbpYx1uJU+tJK8jNv8AifK0w/4nu8WGxFM8+KmfnE/8e4+N4AAfWmgAAAAAAAAAAAAAM7E4i3bVyVKtiysaIr0rQSTKxFXZFckbV4oqp7mf+0zMf0Vkv932v+2S95OV2s5dfqqV1/skkNZ/hIZz+aYn/YXf/MJaRpuqbmMxU4iu1ZoieHTnOnONUTZbDXKnD4upZq9Xl0/ia8sPPhx5cOo1OW3Ju+3pyT6zAO08TvEi9qH4X4yGpF8B1+l8JHMzl1+jz59aZ++3Rbttt6r6+3FkZWNiq5VbibsaVdIAAGUPZWhfK9kcbHSSSvayOONqve971RrWMa3u5yqqIiJ3VVPWZ2AyclK3VuRIx0tC1BZibIjljdJBI2ViPRrkcrFcxN9lRdt+6B5VrpOnNn/tMzH9FZL/AHfa/wC2eq3pXKQxulmxt+KKJvKSWWlYZGxqernPcxEan41JQ/wkM5/NMT/sLv8A5h2tLXVvP6Q1BZuR1on12SQMbUZKxis6cEm7kllequ3evoqexLSFNcxuNs6TctxETMRz6zorEACK6AAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAABkY6nLZmhrwMdLPaljhgiZ3dJLK9GRsan8pXORP0ltfHm9FozRuP0vSkRL+WiVLs0Sqjlj3a/IT7+qJLKqQNRf4vmifQON8kWhmWchY1BcRG0tPsckD5O0a3Xxq50iqvbjDArnrv6LNEvsRb46a6fqLOXMiqr8Pz+Hx7HJt06UKuSBNlTdrnbulVF9HTOOIxv41nlGEje1h9K6+k3J/JT5Rv84ls0/d2uLxns6rw28xuosQ1K88yZaltxWvknOfMxi77pFc/xrfVERJOo1EaiI1CHADqcLlmEwlyu7h7cUTXpxaRprprprEba7zvznxYKq6qo0mQAG8iAAAAAAAAAAAAAJ68nf3xl/wA0r/rJCBSevJ398Zf80r/rJCBSU8oVeF9uv/B2kABFaAN7g8pj4IVSfF/G2eTlZLNeniro1UTg19aujHv2VFVVSVN99u2xJHmuow18hjIa8UcEUWHjayKFiMY1qWbOyIif6f8Aie6bNSrFcN+mzNPPXfbw+f10Q0ADxthOfhR+BGpvysv2esQYTn4UfgRqb8rL9nrEqVZmvqafep7oMABFZgAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAABkY6nLYmirwMdLNaljhgiZ9KSWVyMjY3/OVzkT9JjljPJDoVlnIWNQW0a2np9jkgfJska3Xxq50iqvbjDAqvXf0WaJU9CqzvNLeVYG5i69+GNo6zyiPOU7dE11RS7Dx7uRaN0ZQ0tTkat7LxObelZ9J0Kr1MhN9aNllekDUd/F80T6BUM7nx010/UWcuZHdyQK5IKEbt/3OnBu2FNl7tV27pVT2dM84Y0vsvldzL8DHp97tyZrrn/arf6cvJO/XFdW3KOQADomEAAAAAAAAAAAAAAABPXk7++Mv+aV/1khApPXk7++Mv+aV/wBZIQKSnlCrwvt1/wCDtIACK0CbPN9/6pjv6pZ9pskJk2eb7/1THf1Sz7TZJRylW4j22z+1X8ITABFZBOfhR+BGpvysv2esQYTn4UfgRqb8rL9nrEqVZmvqafep7oMABFZgAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAMjG0pbM0NeBjpZ7U0cMETfpSSyvRkbG7/AMJXORP0ltPHu7Fo3RmP0tUcnx2YhVt6WNe7o14vyMyqvfaWV6QtRf4vmiL8hx3ki0MyzkLGoLiI2lp9jkgkk2SNbro1c6RVXttDAqvXf0WaJfYi3x11y/UOdu5Hdy11f0Mex26dOlCrmwJxXu1Xbulcns6Z5xGN/Gs8owkb2sPpXX0m5P5afKN/nEtmn7u1xeM9nDAA7drAAAAAAD3Uun1I+sj1h6jOskStSRY+Sc0jV6K1H8d9lVFTfYsPU8L9FWdL3dU05tUzV8ZaWpZoc8UlmvK5Ykjkmd0eDq/74gcr2brs9U2Ti5WhXIEh+AOmMNmcsmNzM2Rrx2oJnV7OOWtxgfWifamkufEMd+9krwzKqsTdFRFXtubTT+hcJm8xahws2XTDYTDW8vlJ78dV+Smgx6udO2hBBtHzk51o2JIu6LI5zt9uAEUAkDAWNL3LdSpYx2Ux9eexHFJkIczDbsRNkcjOpNDJQZFM1N91SNIl9V77cV0PidhocdnMzj6/P4fGZjI06/Udyk6NW3NBFzciJyfwY3ddk3XcDnQAAAAE9eTv74y/5pX/AFkhApPXk7++Mv8Amlf9ZIQKSnlCrwvt1/4O0gAIrRv9NalbSjVi43GXXLL1ElyFaWZ7flaiRojZms6fy77K1e7l/wBB0+ofFy1kXxyX8Xhbb4W8I3TU7HJrN+XBHMsovHdVXbf3X6yOQe6tevCWa6uOqNwAHjYCc/Cj8CNTflZfs9YgwnPwo/AjU35WX7PWJUqzNfU0+9T3QYACKzAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAA9+Opy2Joq8DHSTWZY4YYmJu6SWVyMjY1PdyucifpPQWM8kGhW2sjY1BbRraen2OSB8myRrdfGqueqr24wwK567+izRL7FVneaW8qwNzF178MbR1nlEecp26Jrqil2Hj3ci0Zo2hpanI347LxObelZ2c6JVR+QnX1VGyyOSBqO/i+aIvyFQzuPHTXT9RZy5kVV3QV3QoRu7dOlCqthTZe7XO3dK5PZ0zzhzS+y+V3MvwMen3u3JmuufHiq3+nLyTvVxXVtyjkAA6JhAAAAAAsL5JM5BJkMppe85fgdaYueord17WoopVYrV9GOWB9lEXsqubF77FejZaWzU+NvU8hWdxsY61DagXvt1IJGyNR23q1Vbsqe6KoEhJhJdNUNSOsrwyE16bSlJE2RVbG5k+cssa9N+n8N8HX3T1bl12+s1/gFqXLYXI2MziY2T/cXHvs5StK7aOfGPtVKs8bkT5lTq2a790+isaO2VGqi7DzM+KcWq8s23UrSVKNeuxkNeVI2yPsSNa63ZmSJVb1XPRke/Jd2Vod9l3RNZ4I66p4KTNSXabr8eY05cxEdVHcI3yW7FN6rPIj2viiSOCX5o93o7htt9JoS3jta+GmpZkgy2nZdM3Lz1jZk8Zb/edeSRdmyyJGkcUaK7b5nV3NbuvJUTdxB3i9pexhs7lMZandZmpXZEdbfy52WybTR2JOaqqSSRyMeqKq93r3X1XKpt0qyVJ5Jc9PE35/uWtOhCr1REVIH5VtxyoxV3asraqLt3RqL6Y2a1o/I6gXO5OvFa+IycVy3QT5IJa8crF+BRV3VIehG2BFXdeKJvuu6qHJg7nxw1Vi8zl5LuHxMWGpOghjbThZFGjpGIvUndFA1Io3OVUTZvrwRV7qpwwAAAT15O/vjL/mlf8AWSECk9eTv74y/wCaV/1khApKeUKvC+3X/g7SAAitAAAAAAJz8KPwI1N+Vl+z1iDCc/Cj8CNTflZfs9YlSrM19TT71PdBgAIrMAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAZGNpS2Zoa8DFlntTRwwRN+lJLK9GRsbv7q5yJ+ktp493ItG6MoaWqPb8dl4nNvSx9ldGqpJkZlXbfaWV6QNR38XzRPoHHeSLQrLOQs6guI1tLT7HJBJL2jW66Pk6RVXttDAqvXf0WaJfYizx01y/UWcu5Hd3w6v6FCN2/7nShVWwpxX6Ku3dK5PZ0zziMb+NZ5RhI3tYfSuvpNyfyU+Ub/ADiWzT93a4vGezhwAdu1gAAAAAAAAAAAAAAAAAAAABPXk7++Mv8Amlf9ZIQKT15O/vjL/mlf9ZIQKSnlCrwvt1/4O0gAIrRu8Bpua3FJZdLBUpwPSOW9dkWOHqq1XpDE1jXS2J+KK7pxNc5E2VURFRTdp4c2Z60tvF2qeXiqpyssoPmbahZs5eb6duKOZWfKqIrWrv7J2U2WschhJ9N4aOvPOmUx/OKSmjXdFvVe+W1PJyajeT3dHZ7XKqojW7fKvH3+V+WdupKqQ8uEkFttrb06KQPenL8XWZB+nYlpHJWXcReizXejbh12mNpiPrv4TG37owB1Xi9BBFnctHW4pCy/OjWs24tdy3lY3bsjWyK9u3ttscqRWFqv0lEV9Y1Cc/Cj8CNTflZfs9YgwnPwo/AjU35WX7PWJUq/NfU0+9T3QYACKzAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAMjHU5bE0VeBjpZrUscMETO7pJZXIyNjU/lK5yJ+kxyxnkh0MyzkLGoLezKen2OSB8myRrcfE5XyK5eyNhgVXrv6LNEvsVWd5pbyrA3MXXvwxtHWeUR5ynbomuqKXYePV2LRmjaGlqb0+Oy8Tm3pWeronbPyMyr7NllekDUX+L5oi/IVDO48dNdP1FnLmRVV6Cv6FBioqdOlCrmwJsvdrnbulcn8qZ5w5pfZfK68vwMTf3u3JmuufHiq3+nLyTvVxXVtyjkAA6JhAAAAAAAAAAAAAAAAAAAAAE9eTv74y/5pX/WSECk9eTv74y/5pX/WSECkp5Qq8L7df+DtIACK0frGqqoiIqq5URERN1VV7IiInqpMOMvx6NozNRWP1NlYEa5qIjkw9WREexsqru1bblRknT9lSPkmzfnyfCPw4n+DblmW8dBkJ0//AJiXbKcabFRUW86JjXc7X/ttdsjN+a7uRqJ4r4D3JZVfNnMS5ZZFdLKtmWWRyudu9682Jzeqqq93JuvuSiJU2JxuGu1zbuVaUxzjrPT9u8/pzhiWRz3Oc5yuc9yuc5yqrnOVd1c5V7qqr33PE3/iLZjmy2RfC1jYfjrDYGxo1GJDHI6OLijURNuDWr2+s0BFbW6uKmJ00Cc/Cj8CNTflZfs9YgwnPwo/AjU35WX7PWJUq/NfU0+9T3QYACKzAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAABkY6nLZmirwMdJNaljhgib9KSWV6MjY3/OVzkT9JbTx7uRaN0ZQ0tUe347LxObdmjVUV0e7ZMjMvvxlkckDUd/Fq9P4Bx/ki0KyzkLGoLjWtpYBjkgkkVEj+NdHydIqr24wwK567+izRKnoRX4566fqLOXMju7oK/oUI3dljpwqrYU2/gudu6VU9nTPOIxv41nlGEje1h9K6+k3J/JT5Rv84ls0/d2uLxns4cAHbtYAAAAAAAAAAAAAAAAAAAAAAABPXk7++Mv+aV/1khApPnk4bvZy6fXUrp/bJIeP+DRd/pOr/d5v+ZPSZiFDGNsYXHXvS1aa8Pb9ECg73xc8NJ9O/B9a1FZ+6HxHHpRvZw+H6G/Ln679dPT+SpwRCY0XNm9Rfoi5bnWJ/oAAZQA2Gm8Y67dp0muRjr9uvVbI5FVrHWJWRI5UTuqIr99k+oPKqopjWWvJz8KPwI1N+Vl+z1jI/waLv8ASdX+7zf8zqk0FNp/SWfrTTx2FsRyTo+Jj2I1OnDHxVH++7N/0k4pmHP47MsLiKKaLdWs8VPXr+yrQAIOhAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAADyjYrlRqbbuVETk5Gpuq7d3OVEan41XY8QBbbxty9XSmiMbpzG2IprOdg/fVqtIkjJq7/3TIWGSMVUdHLK5IG7qu8avRF+QqSfquVdt1VeKbJ+JN1XZPqTdVX9J+FLkeT05VYqomrjrrqmqqqY0mZmem/ht/bJduccgALpjAAAAAAAAAAAAAAAAAAAAAAAAZNG/PAqrBNLCr0RHLDK+NXInoi8FTdDK/bBf/ntv+9Tf9ZrAEZopneYZN7ITz8evPNNw34daV8nHlty481XbfZPT6kMYAPYiI2gAAeh5xSOY5r2OVr2ORzHtVWua5q7tc1U7oqKiLuh4ADZ/tgv/AM9t/wB6m/6z1z5q5I1zH27L2PTZzH2JXNcn1OartlQwAEPR0dAABMAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAB//2Q==\n",
      "text/html": [
       "\n",
       "        <iframe\n",
       "            width=\"600\"\n",
       "            height=\"400\"\n",
       "            src=\"https://www.youtube.com/embed/fNk_zzaMoSs\"\n",
       "            frameborder=\"0\"\n",
       "            allowfullscreen\n",
       "        ></iframe>\n",
       "        "
      ],
      "text/plain": [
       "<IPython.lib.display.YouTubeVideo at 0x7fcdc81ea070>"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "YouTubeVideo('fNk_zzaMoSs', width=600, height=400)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Basic vector operations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Vector-vector addition "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We used vector-vector addition to define vectors without defining vector-vector addition. Vector-vector addition is an element-wise operation, only defined for vectors of the same size (i.e., number of elements). Consider two vectors of the same size, then: \n",
    "\n",
    "$$\n",
    "\\bf{x} + \\bf{y} = \n",
    "\\begin{bmatrix}\n",
    "x_1\\\\\n",
    "\\vdots\\\\\n",
    "x_n\n",
    "\\end{bmatrix}+\n",
    "\\begin{bmatrix}\n",
    "y_1\\\\\n",
    "\\vdots\\\\\n",
    "y_n\n",
    "\\end{bmatrix} =\n",
    "\\begin{bmatrix}\n",
    "x_1 + y_1\\\\\n",
    "\\vdots\\\\\n",
    "x_n + y_n\n",
    "\\end{bmatrix}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For instance:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\n",
    "\\bf{x} + \\bf{y} = \n",
    "\\begin{bmatrix}\n",
    "1\\\\\n",
    "2\\\\\n",
    "3\n",
    "\\end{bmatrix}+\n",
    "\\begin{bmatrix}\n",
    "1\\\\\n",
    "2\\\\\n",
    "3\n",
    "\\end{bmatrix} =\n",
    "\\begin{bmatrix}\n",
    "1 + 1\\\\\n",
    "2 + 2\\\\\n",
    "3 + 3\n",
    "\\end{bmatrix} =\n",
    "\\begin{bmatrix}\n",
    "2\\\\\n",
    "4\\\\\n",
    "6\n",
    "\\end{bmatrix}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Vector addition has a series of **fundamental properties** worth mentioning:\n",
    "\n",
    "1. Commutativity: $x + y = y + x$\n",
    "2. Associativity: $(x + y) + z = x + (y + z)$\n",
    "3. Adding the zero vector has no effect: $x + 0 = 0 + x = x$\n",
    "4. Substracting a vector from itself returns the zero vector: $x - x = 0$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In `NumPy`, we add two vectors of the same with the `+` operator or the `add` method:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = y = np.array([[1],\n",
    "                  [2],\n",
    "                  [3]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[2],\n",
       "       [4],\n",
       "       [6]])"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x + y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[2],\n",
       "       [4],\n",
       "       [6]])"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.add(x,y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Vector-scalar multiplication"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Vector-scalar multiplication is an element-wise operation. It's defined as:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\n",
    "\\alpha \\bf{x} = \n",
    "\\begin{bmatrix}\n",
    "\\alpha \\bf{x_1}\\\\\n",
    "\\vdots \\\\\n",
    "\\alpha \\bf{x_n}\n",
    "\\end{bmatrix}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Consider $\\alpha = 2$ and $\\bf{x} = \\begin{bmatrix} 1 \\\\ 2 \\\\ 3 \\end{bmatrix}$:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\n",
    "\\alpha \\bf{x} = \n",
    "\\begin{bmatrix}\n",
    "2 \\times 1\\\\\n",
    "2 \\times 2\\\\\n",
    "2 \\times 3\n",
    "\\end{bmatrix} = \n",
    "\\begin{bmatrix}\n",
    "2\\\\\n",
    "4\\\\\n",
    "6\n",
    "\\end{bmatrix}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Vector-scalar multiplication satisfies a series of important properties:\n",
    "\n",
    "1. Associativity: $(\\alpha \\beta) \\bf{x} = \\alpha (\\beta \\bf{x})$\n",
    "2. Left-distributive property: $(\\alpha + \\beta) \\bf{x} = \\alpha \\bf{x} + \\beta \\bf{x}$\n",
    "3. Right-distributive property: $\\bf{x} (\\alpha + \\beta) = \\bf{x} \\alpha + \\bf{x} \\beta$\n",
    "4. Right-distributive property for vector addition: $\\alpha (\\bf{x} + \\bf{y}) = \\alpha \\bf{x} + \\alpha \\bf{y}$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In `NumPy`, we compute scalar-vector multiplication with the `*` operator:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "alpha = 2\n",
    "x = np.array([[1],\n",
    "             [2],\n",
    "             [3]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[2],\n",
       "       [4],\n",
       "       [6]])"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "alpha * x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Linear combinations of vectors"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are only two legal operations with vectors in linear algebra: **addition** and **multiplication by numbers**. When we combine those, we get a **linear combination**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\n",
    "\\alpha \\bf{x} + \\beta \\bf{y} = \n",
    "\\alpha\n",
    "\\begin{bmatrix}\n",
    "x_1 \\\\ \n",
    "x_2\n",
    "\\end{bmatrix}+\n",
    "\\beta\n",
    "\\begin{bmatrix}\n",
    "y_1 \\\\ \n",
    "y_2\n",
    "\\end{bmatrix}=\n",
    "\\begin{bmatrix}\n",
    "\\alpha x_1 + \\alpha x_2\\\\ \n",
    "\\beta y_1 + \\beta y_2\n",
    "\\end{bmatrix}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Consider $\\alpha = 2$, $\\beta = 3$, $\\bf{x}=\\begin{bmatrix}2 \\\\ 3\\end{bmatrix}$, and $\\begin{bmatrix}4 \\\\ 5\\end{bmatrix}$.\n",
    "\n",
    "We obtain:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\n",
    "\\alpha \\bf{x} + \\beta \\bf{y} = \n",
    "2\n",
    "\\begin{bmatrix}\n",
    "2 \\\\ \n",
    "3\n",
    "\\end{bmatrix}+\n",
    "3\n",
    "\\begin{bmatrix}\n",
    "4 \\\\ \n",
    "5\n",
    "\\end{bmatrix}=\n",
    "\\begin{bmatrix}\n",
    "2 \\times 2 + 2 \\times 4\\\\ \n",
    "2 \\times 3 + 3 \\times 5\n",
    "\\end{bmatrix}=\n",
    "\\begin{bmatrix}\n",
    "10 \\\\\n",
    "21\n",
    "\\end{bmatrix}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Another way to express linear combinations you'll see often is with summation notation. Consider a set of vectors $x_1, ..., x_k$ and scalars $\\beta_1, ..., \\beta_k \\in \\mathbb{R}$, then:   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\n",
    "\\sum_{i=1}^k \\beta_i x_i := \\beta_1x_1 + ... + \\beta_kx_k\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that $:=$ means \"*is defined as*\".\n",
    "\n",
    "Linear combinations are the most fundamental operation in linear algebra. Everything in linear algebra results from linear combinations. For instance, linear regression is a linear combination of vectors. **Fig. 2** shows an example of how adding two geometrical vectors looks like for intuition."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In `NumPy`, we do linear combinations as:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "a, b = 2, 3\n",
    "x , y = np.array([[2],[3]]), np.array([[4], [5]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[16],\n",
       "       [21]])"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a*x + b*y"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Vector-vector multiplication: dot product"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We covered vector addition and multiplication by scalars. Now I will define vector-vector multiplication, commonly known as a **dot product** or **inner product**. The dot product of $\\bf{x}$ and $\\bf{y}$ is defined as: \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\n",
    "\\bf{x} \\cdot \\bf{y} :=\n",
    "\\begin{bmatrix}\n",
    "x_1 \\\\\n",
    "x_2\n",
    "\\end{bmatrix}^T\n",
    "\\begin{bmatrix}\n",
    "y_1 \\\\\n",
    "y_2\n",
    "\\end{bmatrix} =\n",
    "\\begin{bmatrix}\n",
    "x_1 & x_2\n",
    "\\end{bmatrix}\n",
    "\\begin{bmatrix}\n",
    "y_1 \\\\\n",
    "y_2\n",
    "\\end{bmatrix} =\n",
    "x_1 \\times y_1 + x_2 \\times y_2 \n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Where the $T$ superscript denotes the transpose of the vector. Transposing a vector just means to \"flip\" the column vector to a row vector counterclockwise. For instance:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\n",
    "\\bf{x} \\cdot \\bf{y} =\n",
    "\\begin{bmatrix}\n",
    "-2 \\\\\n",
    "2\n",
    "\\end{bmatrix}\n",
    "\\begin{bmatrix}\n",
    "4 \\\\\n",
    "-3\n",
    "\\end{bmatrix} =\n",
    "\\begin{bmatrix}\n",
    "-2 & 2\n",
    "\\end{bmatrix}\n",
    "\\begin{bmatrix}\n",
    "4 \\\\\n",
    "-3\n",
    "\\end{bmatrix} =\n",
    "-2 \\times 4 + 2 \\times -3 = (-8) + (-6) = -14  \n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Dot products are so important in machine learning, that after a while they become second nature for practitioners.\n",
    "\n",
    "To multiply two vectors with dimensions (rows=2, cols=1) in `Numpy`, we need to transpose the first vector at using the `@` operator:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "x, y = np.array([[-2],[2]]), np.array([[4],[-3]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[-14]])"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x.T @ y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "produto vetorial\n",
    "produto misto\n",
    "matrizes\n",
    "determinantes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Matrices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Libraries for this section \n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import altair as alt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Matrices are as fundamental as vectors in machine learning. With vectors, we can represent single variables as sets of numbers or instances. With matrices, we can represent sets of variables. In this sense, a matrix is simply an ordered **collection of vectors**. Conventionally, column vectors, but it's always wise to pay attention to the authors' notation when reading matrices. Since computer screens operate in two dimensions, matrices are the way in which we interact with data in practice.\n",
    "\n",
    "More formally, we represent a matrix with a italicized upper-case letter like $\\textit{A}$. In two dimensions, we say the matrix $\\textit{A}$ has $m$ rows and $n$ columns. Each entry of $\\textit{A}$ is defined as $a_{ij}$, $i=1,..., m,$ and $j=1,...,n$. A matrix $\\textit{A} \\in \\mathbb{R^{m\\times n}}$ is defines as:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\n",
    "A :=\n",
    "\\begin{bmatrix}\n",
    "a_{11} & a_{12} & \\cdots & a_{1n}\\\\\n",
    "a_{21} & a_{22} & \\cdots & a_{2n}\\\\\n",
    "\\vdots & \\vdots & \\ddots & \\vdots\\\\\n",
    "a_{m1} & a_{m2} & \\cdots & a_{mn}\n",
    "\\end{bmatrix},\n",
    "a_{ij} \\in \\mathbb{R}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In `Numpy`, we construct matrices with the `array` method: "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A = np.array([[0,2],  # 1st row\n",
    "              [1,4]]) # 2nd row\n",
    "\n",
    "print(f'a 2x2 Matrix:\\n{A}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Basic Matrix operations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Matrix-matrix addition "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We add matrices in a element-wise fashion. The sum of $\\textit{A} \\in \\mathbb{R}^{m\\times n}$ and $\\textit{B} \\in \\mathbb{R}^{m\\times n}$ is defined as:\n",
    "\n",
    "$$\n",
    "\\textit{A} + \\textit{B} := \n",
    "\\begin{bmatrix}\n",
    "a_{11} + b_{11} & \\cdots & a_{1n} + b_{1n} \\\\\n",
    "\\vdots & \\ddots & \\vdots \\\\\n",
    "a_{m1} + b_{m1} & \\cdots & a_{mn} + b_{mn}\n",
    "\\end{bmatrix}\n",
    "\\in \\mathbb{R^{m\\times n}}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For instance: \n",
    "$$\n",
    "\\textit{A} = \n",
    "\\begin{bmatrix}\n",
    "0 & 2 \\\\\n",
    "1 & 4\n",
    "\\end{bmatrix} + \n",
    "\\textit{B} = \n",
    "\\begin{bmatrix}\n",
    "3 & 1 \\\\\n",
    "-3 & 2\n",
    "\\end{bmatrix}=\n",
    "\\begin{bmatrix}\n",
    "0+3 & 2+1 \\\\\n",
    "3+(-3) & 2+2\n",
    "\\end{bmatrix}=\n",
    "\\begin{bmatrix}\n",
    "3 & 3 \\\\\n",
    "-2 & 6\n",
    "\\end{bmatrix}\n",
    "$$ "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In `Numpy`, we add matrices with the `+` operator or `add` method:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "A = np.array([[0,2],\n",
    "              [1,4]])\n",
    "B = np.array([[3,1],\n",
    "              [-3,2]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 3,  3],\n",
       "       [-2,  6]])"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "A + B"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 3,  3],\n",
       "       [-2,  6]])"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.add(A, B)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Determinant"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If matrices had personality, the **determinant** would be the personality trait that reveals most information about the matrix character. The determinant of a matrix is a single number that tells **whether a matrix is invertible or singular**, this is, whether its columns are linearly independent or not, which is one of the most important things you can learn about a matrix. Actually, the name \"determinant\" refers to the property of \"determining\" if the matrix is singular or not. Specifically, for an square matrix $\\textit{A} \\in \\mathbb{R}^{n \\times n}$, a determinant equal to $0$, denoted as $\\text{det}(\\textit{A}=0)$, implies *the matrix is singular* (i.e., noninvertible), whereas a determinant equal to $1$,  denoted as $\\text{det}(\\textit{A})=1$, implies the *matrix is not singular* (i.e., invertible). Although determinants can reveal if matrices are singular with a single number, it's not used for large matrices as Gaussian Elimination is faster. \n",
    "\n",
    "Recall that matrices can be thought of as function action on vectors or other matrices. Thus, the determinant can also be considered a linear mapping of a matrix $\\textit{A}$ onto a single number. But, what does that number mean? So far, we have defined determinants based on their utility of determining matrix invertibility. Before going into the calculation of determinants, let's examine determinants from a geometrical perspective to gain insight into the meaning of determinants."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Determinant as measures of volume"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From a geometric perspective, determinants indicate the $\\textit{sign}$ **area of a parallelogram** (e.g., a rectangular area) and the $\\textit{sign}$ **volume of the parallelepiped**, for a matrix whose columns consist of the basis vectors in Euclidean space. \n",
    "\n",
    "Let's parse out the above phrase: the $\\textit{sign}$ area indicates the absolute value of the area, and the $\\textit{sign}$ volume equals the absolute value of the volume. You may be wondering why we need to take the absolute value since real-life objects can't have negative area or volume. In linear algebra, we say the area of a parallelogram is **negative** when the vectors forming the figure are *clockwise oriented* (i.e., negatively oriented), and **positive** when the vectors forming the figure are *counterclockwise oriented* (i.e., positively oriented). \n",
    "\n",
    "Here is an example of a matrix $\\textit{A}$ with vectors *clockwise* or *negatively* oriented:\n",
    "\n",
    "$$\n",
    "\\textit{A} =\n",
    "\\begin{bmatrix}\n",
    "0 & 2 \\\\\n",
    "2 & 0\n",
    "\\end{bmatrix}\n",
    "$$\n",
    "\n",
    "The elements of the first column, indicate the first vector of the matrix, while the elements of the second column, the second vector of the matrix. Therefore, when we measure the area of the parallelogram formed by the pair of vectors, we move from left to right, i.e., *clockwise*, meaning that the vectors are **negatively oriented**, and the **area of the matrix will be negative**. \n",
    "\n",
    "Here is the same matrix $\\textit{A}$ with vectors *counterclockwise* or *positively* oriented:\n",
    "\n",
    "$$\n",
    "\\textit{A} =\n",
    "\\begin{bmatrix}\n",
    "2 & 0 \\\\\n",
    "0 & 2\n",
    "\\end{bmatrix}\n",
    "$$\n",
    "\n",
    "Again, the elements of the *first column*, indicate the *first vector* of the matrix, while the elements of the *second column*, the *second vector* of the matrix. Therefore, when we measure the area of the parallelogram formed by the pair of vectors, we move from *right to left*, i.e., *counterclockwise*, meaning that the vectors are **positively oriented**, and the **area of the matrix will be positive**. \n",
    "\n",
    "The figure below exemplifies what I just said."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<center>\n",
    "<img src=\"./images/b-determinant-orientation.svg\">\n",
    "<center/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The situation for the $\\textit{sign}$ volume of the parallelepiped is no different: when the vectors are *counterclockwise* oriented, we say the vectors are *positively oriented* (i.e., positive volume); when the vectors are *clockwise* oriented, we say the vectors are *negatively oriented* (i.e., negative volume)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The 2 X 2 determinant"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Recall that matrices are invertible or nonsingular when their columns are linearly independent. By extension, the determinant allow us to whether the columns of a matrix a linearly independent. To understand this method, let's examine the $2 \\times 2$ special case first.\n",
    "\n",
    "Consider a square matrix as:\n",
    "\n",
    "$$ \\textit{A} =\n",
    "\\begin{bmatrix}\n",
    "1 & 4 \\\\\n",
    "2 & 8\n",
    "\\end{bmatrix}\n",
    "$$\n",
    "\n",
    "How can we decide whether the columns are linearly independent? A strategy that I often use in simple cases like this, is just to examine whether the second column equals the first column times some factor. In the case of $\\textit{A}$ is easy to see that the second column equals four times the first column, so the columns are linearly *dependent*. We can express such criteria by comparing the *elementwise division* between each element of the second column by each element of the first column as:\n",
    "\n",
    "$$\n",
    "\\begin{bmatrix}\n",
    "\\frac{4}{1} =\n",
    "\\frac{8}{2}\n",
    "\\end{bmatrix}\n",
    "= \n",
    "\\begin{bmatrix}\n",
    "4 =\n",
    "4\n",
    "\\end{bmatrix}\n",
    "$$\n",
    "\n",
    "We obtain that both entries equal $4$, meaning that the second column can be divided exactly by the first column (i.e., linearly *dependent*).\n",
    "\n",
    "\n",
    "Consider this matrix now:\n",
    "\n",
    "$$ \\textit{B} =\n",
    "\\begin{bmatrix}\n",
    "0 & 4 \\\\\n",
    "0 & 8\n",
    "\\end{bmatrix}\n",
    "$$\n",
    "\n",
    "Let's try again our method for $\\textit{B}$:\n",
    "\n",
    "$$\n",
    "\\begin{bmatrix}\n",
    "\\frac{4}{0} =\n",
    "\\frac{8}{0}\n",
    "\\end{bmatrix}\n",
    "= \n",
    "\\begin{bmatrix}\n",
    "\\textit{undef} =\n",
    "\\textit{undef}\n",
    "\\end{bmatrix}\n",
    "$$\n",
    "\n",
    "Now we got into a problem because division by $0$ is undefined, so we can determine the relationship between columns of $\\textit{B}$. Yet, by inspection, we can see the first column is simply $0$ times the second column, therefore linearly dependent. Here is when **determinants** come to the rescue. \n",
    "\n",
    "Consider the generic matrix:\n",
    "\n",
    "$$\n",
    "\\begin{bmatrix}\n",
    "a & b \\\\\n",
    "c & d\n",
    "\\end{bmatrix}\n",
    "$$\n",
    "\n",
    "According to our previous strategy, we had:\n",
    "\n",
    "$$\n",
    "\\frac{b}{a} = \\frac{d}{c}\n",
    "$$\n",
    "\n",
    "This is, we tested the elementwise division of the second column by the first column. Before, we failed because of division, so we probably want a method that does not involve it. Notice that we can rearrange our expression as:\n",
    "\n",
    "$$\n",
    "ad = bc\n",
    "$$\n",
    "\n",
    "Let's try again with this method for $\\textit{A}$: \n",
    "\n",
    "$$\n",
    "\\begin{bmatrix}\n",
    "1 \\times 8 = 4 \\times 2 \\\\\n",
    "\\end{bmatrix}\n",
    "\\begin{bmatrix}\n",
    "8 = 8\n",
    "\\end{bmatrix}\n",
    "$$\n",
    "\n",
    "And for $\\textit{B}$:\n",
    "\n",
    "$$\n",
    "\\begin{bmatrix}\n",
    "0 \\times 8 = 4 \\times 0 \\\\\n",
    "\\end{bmatrix}\n",
    "\\begin{bmatrix}\n",
    "0 = 0\n",
    "\\end{bmatrix}\n",
    "$$\n",
    "\n",
    "It works. Indeed, $ad = bc$ are equal for both matrices, $\\textit{A}$ and $\\textit{B}$, meaning their columns are linearly dependent. Finally, notice that we can rearange all the terms on one side of the equation as: \n",
    "\n",
    "$$ \n",
    "(ad) - (bc)=0 \n",
    "$$\n",
    "\n",
    "There you go: the above expression is what is known as the **determinant of a matrix**. We denote the determinant as:\n",
    "\n",
    "$$\\vert \\textit{A} \\vert = \n",
    "\\begin{vmatrix}\n",
    "a & b \\\\\n",
    "c & d\n",
    "\\end{vmatrix} =\n",
    "(ad) - (bc)\n",
    "$$\n",
    "\n",
    "Or \n",
    "\n",
    "$$\\textit{det (A)} = \n",
    "\\begin{vmatrix}\n",
    "a & b \\\\\n",
    "c & d\n",
    "\\end{vmatrix} =\n",
    "(ad) - (bc)\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The N X N determinant"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As matrices larger, computing the determinant gets more complicated. Consider the $3 \\times 3$ case as:\n",
    "\n",
    "$$\\vert \\textit{A} \\vert = \n",
    "\\begin{vmatrix}\n",
    "a & b & c\\\\\n",
    "d & e & f\\\\\n",
    "g & h & i\n",
    "\\end{vmatrix}\n",
    "$$\n",
    "\n",
    "The problem now is that linearly independent columns can be either: (1) multiples of another column, and (2) linear combinations of pairs of columns. The determinant for a $3 \\times 3$ is:\n",
    "\n",
    "$$\n",
    "\\vert \\textit{A} \\vert = aei - afh + bfg - bdi + cdh - ceg \n",
    "$$\n",
    "\n",
    "Such expression is hard to memorize, and it will get even more complicated for larger matrices. For instance, the $4 \\times 4$ entails 24 terms. As with most things in mathematics, there is a general formula to express the determinant compactly, which is known as the Leibniz's formula:\n",
    "\n",
    "$$\n",
    "\\vert \\textit{A} \\vert = \\sum_{\\sigma} \\textit{sign}(\\sigma) \\prod_{i=1}^n a_{\\sigma(i),i} \n",
    "$$\n",
    "\n",
    "Where $\\sigma$ computes the permutation for the rows and columns vectors of the matrix. Is of little importance for us to break down the meaning of this formula since we are interested in its applicability and conceptual value. What is important to notice, is that for an arbitrary square $n \\times n$ matrix, we will have $n!$ terms to sum over. For instance, for a $10 \\times 10$ matrix, $10! = 3,628,800$, which is a gigantic number considering the size of the matrix. In machine learning, we care about matrices with thousands or even millions of columns, so there is no use for such formula. Nonetheless, this does not mean that the determinant is useless, but the direct calculation with the above algebraic expression is not used. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Determinants as scaling factors"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When we think in matrices as linear mappings, this is, as functions applied to vectors (or vectors spaces), the determinant acquires an intuitive geometrical interpretation: **as the factor by which areas are scaled under a mapping**. Plainly, if you do a mapping or transformation, and  the area increases by a factor of $3$, then the determinant of the transformation matrix equals $3$. Consider the matrix $\\textit{A}$ and the basis vector $\\textbf{x}$:\n",
    "\n",
    "$$\n",
    "\\textit{A} =\n",
    "\\begin{bmatrix}\n",
    "4 & 0 \\\\\n",
    "0 & 3\n",
    "\\end{bmatrix}\n",
    "$$\n",
    "\n",
    "$$ \\textbf{x} =\n",
    "\\begin{bmatrix}\n",
    "1 & 1 \n",
    "\\end{bmatrix}\n",
    "$$\n",
    "\n",
    "Is easy to see that the parallelogram formed by the basis vectors of $\\textbf{x}$ is $1 \\times 1 = 1$. When we apply $\\textit{A}\\textbf{x}$, we get: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "A = np.array([[4, 0],\n",
    "              [0, 3]])\n",
    "\n",
    "x = np.array([1,1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([4, 3])"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "A @ x.T"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Meaning that the vertical axis was scaled by $4$ and the horizontal axis by $3$, hence, the new parallelogram has area $4 \\times 3 = 12$. Since the new area has increased by a factor of $12$, the determinant $\\vert \\textit{A} \\vert =  12$. Although we exemplified this with the basis vectors in $\\textit{x}$, the determinant of $\\textit{A}$ for mappings of the entire vector space. The figure below visually illustrates this idea. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<center>\n",
    "<img src=\"./images/b-determinant-scaling.svg\">\n",
    "<center/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The importance of determinants"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Considering that calculating the determinant is not computationally feasible for large matrices and that we can determine linear independence via Gaussian Elimination, you may be wondering what's the point of learning about determinants in the first place. I also asked myself more than once. It turns out that determinants play a crucial conceptual role in other topics in matrix decomposition, particularly eigenvalues and eigenvectors. Some books I reviewed devote a ton of space to determinants, whereas others (like Strang's Intro to Linear Algebra) do not. In any case, we study determinants mostly because of its conceptual value to better understand linear algebra and matrix decomposition."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
