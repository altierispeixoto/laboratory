{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "541b1b39",
   "metadata": {},
   "source": [
    "# Unlocking Network Depth: ResNet Architecture\n",
    "\n",
    "You have already built a solid foundation in creating and training models that are often linear, with data flowing sequentially through a stack of layers. But what happens when this straightforward approach isn't enough? A logical assumption is that making a network deeper should improve its performance; however, in practice, a frustrating problem often emerges. As networks get very deep, their performance can stagnate or even worsen. This is frequently due to the **vanishing gradient problem**, where the learning signal that guides the model's updates becomes progressively weaker as it travels backward through many layers, hindering the entire network's ability to learn.\n",
    "\n",
    "The solution lies not in a complex new algorithm, but in an elegant architectural design that gives you direct control over the flow of data. In this lab, you will see this principle in action by building and comparing two distinct deep learning models. This hands-on experiment will provide a clear, intuitive understanding of how a simple architectural change can overcome a fundamental training obstacle, enabling the creation of deeper, more powerful models.\n",
    "\n",
    "In this lab, you will:\n",
    "\n",
    "* Build a `PlainCNN`, a standard sequential model that will serve as a performance baseline.\n",
    "\n",
    "* Implement a `SimpleResNet`, an advanced architecture that introduces **residual \"skip\" connections**.\n",
    "\n",
    "* Train both models on the **Skyview Multi-Landscape Aerial Imagery Dataset** to classify different types of landscapes.\n",
    "\n",
    "* Compare the performance of the two models to directly observe the impact of the skip connection on training stability and final accuracy.\n",
    "\n",
    "Let's get started!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2de4b996",
   "metadata": {},
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d16e8e03",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "import torchinfo\n",
    "from torchvision import transforms\n",
    "\n",
    "import helper_utils\n",
    "\n",
    "# Set seed\n",
    "SEED = 42"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29fb64f9",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d1507bd9",
   "metadata": {},
   "source": [
    "## A Birds Eye View: The Skyview Dataset\n",
    "\n",
    "Imagine you are building an automated system to map an entire region from the sky. Before you can track urban growth or monitor environmental shifts, your system needs a fundamental skill: the ability to look at an aerial image and instantly recognize what it sees. Is it a dense forest, a bustling harbor, or a sprawling residential area? This is precisely the challenge you will tackle.\n",
    "\n",
    "For this lab, you will work with the [Skyview Multi-Landscape Aerial Imagery Dataset](https://www.kaggle.com/datasets/ankit1743/skyview-an-aerial-landscape-dataset), a collection of **12,000 satellite images**, each of **256x256 pixels**. You are to train a model that can accurately classify these images into **15 distinct categories**. The dataset is rich and varied, featuring everything from natural landscapes like `Forest`, `River` and `Mountain` to man made structures such as `Airport`, `Highway`, and `Residential` zones.\n",
    "\n",
    "Developing a robust landscape classifier is an essential step in many real world geospatial applications. The model you build could serve as the core component for urban planning tools, automated environmental monitoring systems, or even logistics and supply chain optimization. You are not just sorting pictures; you are teaching a machine to interpret the world, a foundational capability for countless advanced AI solutions.\n",
    "\n",
    "* Run the next cells below to see an overview of this dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a911f909-6fff-4a36-88e8-d838ed993731",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Set the path to the root directory of the image dataset.\n",
    "dataset_path = \"./Aerial_Landscapes/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fff3d6ef-a6b5-45c5-8829-e594b43483ed",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Display the image count statistics for each class.\n",
    "helper_utils.display_dataset_stats(dataset_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2093961e-bd31-4376-bccf-27cfd5b1b26f",
   "metadata": {},
   "source": [
    "### Assemble Your Data Pipeline\n",
    "\n",
    "* First, define your pipelines of transformations for training and validation data\n",
    "    * Use the pre-calculated `mean` and `std` of this dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "61fbd098",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Pre-calculated mean and std of this dataset\n",
    "mean = [0.378, 0.393, 0.345]\n",
    "std = [0.205, 0.173, 0.170]\n",
    "\n",
    "# Transformations for the training set (with augmentation)\n",
    "train_transform = transforms.Compose([\n",
    "    transforms.RandomResizedCrop(size=(100, 100), scale=(0.8, 1.0)),\n",
    "    transforms.RandomHorizontalFlip(p=0.5),\n",
    "    transforms.RandomVerticalFlip(p=0.5),\n",
    "    transforms.RandomRotation(15),\n",
    "    transforms.ColorJitter(brightness=0.2, contrast=0.2, saturation=0.2, hue=0.1),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=mean, std=std),\n",
    "])\n",
    "\n",
    "# Transformations for validation set (no augmentation)\n",
    "val_transform = transforms.Compose([\n",
    "    transforms.Resize((100, 100)),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(\n",
    "        mean=mean,\n",
    "        std=std,\n",
    "    ),\n",
    "])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0716cfc3-4269-4390-9765-bcdab5c2f57b",
   "metadata": {},
   "source": [
    "* Now, you will split the data into an 80% training set and a 20% validation set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "03e2d324-7c8d-473e-a735-2e7ab22047ce",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Create the training and validation datasets by splitting the main dataset.\n",
    "train_dataset, val_dataset = helper_utils.create_datasets(\n",
    "    dataset_path, \n",
    "    train_transform,\n",
    "    val_transform,\n",
    "    train_split=0.8,\n",
    "    seed=SEED\n",
    ")\n",
    "\n",
    "# Determine the number of unique classes from the dataset's properties.\n",
    "num_classes = len(train_dataset.classes)\n",
    "\n",
    "# Print a summary of the dataset split.\n",
    "print(f\"Total Number of Classes:  {num_classes}\")     \n",
    "print(f\"Training set size:        {len(train_dataset)}\")\n",
    "print(f\"Validation set size:      {len(val_dataset)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "38743e86-1d54-414c-962c-73239230cb54",
   "metadata": {},
   "source": [
    "<br>\n",
    "\n",
    "* Next, you will create the dataloaders for your training and validation sets, defining a batch size of 32."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "09f3893c",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Define the number of images to process in each batch.\n",
    "batch_size = 32\n",
    "\n",
    "# Create the training and validation DataLoaders using the helper function.\n",
    "train_loader, val_loader = helper_utils.create_dataloaders(train_dataset, val_dataset, batch_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f3f6b01e-79f4-4128-a66c-56d452eea75a",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Visualize Training Samples\n",
    "\n",
    "* Run the next cell to visualize your training samples."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4976d1b5-ed91-4395-9e95-c168355d69fd",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Display the sample images from train set\n",
    "helper_utils.show_sample_images(train_dataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a96e602-3fab-4cee-b21f-d4c5179e0e6c",
   "metadata": {},
   "source": [
    "## Architecting Your Neural Networks\n",
    "\n",
    "You've prepared the data, and now it's time to construct the engine that will learn to classify it. In this section, you will build two distinct deep learning models from scratch. This hands on comparison is designed to give you a clear and intuitive understanding of a pivotal innovation in neural network design.\n",
    "\n",
    "A logical assumption in deep learning is that making a network deeper by adding more layers should always improve its performance. However, in practice, a frustrating problem often emerges: as networks get very deep, their performance can stagnate or even get worse. This is often due to the **vanishing gradient problem**, where the learning signal that guides the model's updates becomes progressively weaker as it travels backward through the layers. The earliest layers end up learning very little, hindering the entire network.\n",
    "\n",
    "How do you solve this? The answer lies not in a complex new algorithm, but in a simple, brilliant architectural design. You are about to implement this solution and see firsthand how it enables the creation of deeper, more powerful models. This is the kind of engineering insight that separates standard models from state of the art ones in professional applications, from medical imaging to the geospatial analysis you are performing now.\n",
    "\n",
    "### The `PlainCNN`: A Baseline for Comparison\n",
    "\n",
    "First, you will build the **baseline model**. The architecture will likely look familiar, as it follows the standard and intuitive approach of stacking convolutional blocks **sequentially**. Each block hands off its output to the next, allowing the network to learn progressively more abstract features.\n",
    "\n",
    "This `PlainCNN` will serve as the control in your experiment. Its performance is the benchmark you will use to truly appreciate the architectural improvements of the **ResNet style model** you will build next.\n",
    "\n",
    "#### The Core Component: A Plain Block\n",
    "\n",
    "The `PlainBlock` is the simple, repeatable building unit for your `PlainCNN`.\n",
    "\n",
    "* **`__init__`**: This is the constructor where you define the block's layers. It initializes two **convolutional layers** and their corresponding **batch normalization** layers, which will make up the block's architecture.\n",
    "\n",
    "* **`forward`**: This method defines the data's path through the block. It passes the input through the first convolution and activation, then through the second, executing the layers in a **simple, sequential order**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "afc29573-e54a-477d-ac5e-efce441bdb89",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "class PlainBlock(nn.Module):\n",
    "    \"\"\"\n",
    "    A basic two-layer convolutional block without skip connections.\n",
    "\n",
    "    Args:\n",
    "        in_channels (int): The number of channels in the input feature map.\n",
    "        out_channels (int): The number of channels produced by the convolutions.\n",
    "        stride (int, optional): The stride for the first convolutional layer,\n",
    "                                used for downsampling. Defaults to 1.\n",
    "    \"\"\"\n",
    "    def __init__(self, in_channels, out_channels, stride=1):\n",
    "        # Initialize the parent nn.Module.\n",
    "        super(PlainBlock, self).__init__()\n",
    "\n",
    "        # First convolutional layer, which handles input channels and potential downsampling.\n",
    "        self.conv1 = nn.Conv2d(in_channels, out_channels, kernel_size=3, stride=stride, padding=1, bias=False)\n",
    "        self.bn1 = nn.BatchNorm2d(out_channels)\n",
    "\n",
    "        # Second convolutional layer.\n",
    "        self.conv2 = nn.Conv2d(out_channels, out_channels, kernel_size=3, stride=1, padding=1, bias=False)\n",
    "        self.bn2 = nn.BatchNorm2d(out_channels)\n",
    "\n",
    "    def forward(self, x):\n",
    "        \"\"\"\n",
    "        Defines the forward pass for the PlainBlock.\n",
    "\n",
    "        Args:\n",
    "            x (torch.Tensor): The input tensor.\n",
    "\n",
    "        Returns:\n",
    "            torch.Tensor: The output tensor after two convolutions.\n",
    "        \"\"\"\n",
    "        # Apply the first convolution, batch normalization, and ReLU activation.\n",
    "        out = F.relu(self.bn1(self.conv1(x)))\n",
    "        \n",
    "        # Apply the second convolution and batch normalization.\n",
    "        # Note: A final activation is typically applied after this block in the main network.\n",
    "        out = self.bn2(self.conv2(out))\n",
    "        \n",
    "        # Return the output feature map.\n",
    "        return out"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a9d7ac8-ec79-4046-99f3-a3ab2246f992",
   "metadata": {},
   "source": [
    "#### Constructing the `PlainCNN` Architecture\n",
    "\n",
    "Now you'll use the `PlainBlock` as a blueprint to construct the complete `PlainCNN` model.\n",
    "\n",
    "* **`__init__`**: This constructor builds the **full network blueprint**. It defines the initial layer, uses a helper method to stack your `PlainBlocks` into three main stages, and adds the final classification head.\n",
    "\n",
    "* **`forward`**: This method defines the **end to end data flow**. It passes an image tensor through each stage of the network in order, from the initial convolution to the final prediction layer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12d08fd0-132c-468e-bc5a-7233fd08eae2",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "class PlainCNN(nn.Module):\n",
    "    \"\"\"\n",
    "    A plain Convolutional Neural Network for image classification.\n",
    "\n",
    "    This network is constructed from a series of basic convolutional blocks\n",
    "    (PlainBlock) without using residual (skip) connections. It features an\n",
    "    initial convolution, followed by several layers of stacked blocks that\n",
    "    progressively increase channel depth and reduce spatial dimensions, and\n",
    "    concludes with a classification head.\n",
    "\n",
    "    Args:\n",
    "        num_classes (int, optional): The number of output classes for the final\n",
    "                                     classification layer. Defaults to 5.\n",
    "        num_blocks (list of int, optional): A list defining the number of\n",
    "                                            PlainBlocks in each of the three\n",
    "                                            main layers. Defaults to [2, 2, 2].\n",
    "    \"\"\"\n",
    "    def __init__(self, num_classes=5, num_blocks=[2, 2, 2]):\n",
    "        # Initialize the parent nn.Module.\n",
    "        super(PlainCNN, self).__init__()\n",
    "\n",
    "        # Initialize the number of input channels for the first main layer.\n",
    "        self.in_channels = 32\n",
    "        \n",
    "        # Initial convolutional block to process the input image.\n",
    "        self.initial_block = nn.Sequential(\n",
    "            nn.Conv2d(3, 32, kernel_size=3, stride=1, padding=1, bias=False),\n",
    "            nn.BatchNorm2d(32),\n",
    "            nn.ReLU(inplace=True),\n",
    "        )\n",
    "\n",
    "        # Define the main layers of the network by stacking PlainBlocks.\n",
    "        self.layer1 = self._make_layer(32, num_blocks[0], stride=1)\n",
    "        self.layer2 = self._make_layer(64, num_blocks[1], stride=2)\n",
    "        self.layer3 = self._make_layer(128, num_blocks[2], stride=2)\n",
    "\n",
    "        # Final block for global average pooling and classification.\n",
    "        self.final_block = nn.Sequential(\n",
    "            nn.AdaptiveAvgPool2d((1, 1)),\n",
    "            nn.Flatten(),\n",
    "            nn.Linear(128, num_classes),\n",
    "        )\n",
    "\n",
    "    def _make_layer(self, out_channels, num_blocks, stride):\n",
    "        \"\"\"\n",
    "        Builds a layer by stacking multiple PlainBlocks.\n",
    "\n",
    "        Args:\n",
    "            out_channels (int): The number of output channels for the blocks.\n",
    "            num_blocks (int): The number of blocks to stack in this layer.\n",
    "            stride (int): The stride for the first block, used for downsampling.\n",
    "\n",
    "        Returns:\n",
    "            nn.Sequential: A sequential container of the stacked blocks.\n",
    "        \"\"\"\n",
    "        # Initialize an empty list to hold the blocks for this layer.\n",
    "        layers = []\n",
    "        \n",
    "        # The first block in a layer handles downsampling and channel changes.\n",
    "        layers.append(PlainBlock(self.in_channels, out_channels, stride))\n",
    "        \n",
    "        # Update the number of input channels for the subsequent layer.\n",
    "        self.in_channels = out_channels\n",
    "        \n",
    "        # Add the remaining blocks for this layer.\n",
    "        for _ in range(1, num_blocks):\n",
    "            layers.append(PlainBlock(self.in_channels, out_channels))\n",
    "            \n",
    "        # Return the blocks wrapped in a sequential container.\n",
    "        return nn.Sequential(*layers)\n",
    "\n",
    "    def forward(self, x):\n",
    "        \"\"\"\n",
    "        Defines the forward pass for the PlainCNN.\n",
    "\n",
    "        Args:\n",
    "            x (torch.Tensor): The input image tensor.\n",
    "\n",
    "        Returns:\n",
    "            torch.Tensor: The output logits from the final classification layer.\n",
    "        \"\"\"\n",
    "        # Pass the input through the initial block.\n",
    "        out = self.initial_block(x)\n",
    "        \n",
    "        # Pass through the main layers, each followed by a ReLU activation.\n",
    "        out = F.relu(self.layer1(out))\n",
    "        out = F.relu(self.layer2(out))\n",
    "        out = F.relu(self.layer3(out))\n",
    "        \n",
    "        # Pass through the final classification block.\n",
    "        out = self.final_block(out)\n",
    "        \n",
    "        # Return the final output logits.\n",
    "        return out"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cda77512",
   "metadata": {},
   "source": [
    "### The `SimpleResNet`: An Elegant Solution\n",
    "\n",
    "Now for the main event. You'll build a model using the ResNet style of architecture. On the surface, it looks very similar to the `PlainCNN`, but it contains one pivotal change that makes all the difference.\n",
    "\n",
    "Instead of a `PlainBlock`, you'll use a `ResidualBlock`. This block introduces a **\"skip connection\"** or **\"shortcut\"** that runs in parallel to the main convolutional layers. This simple addition is the elegant solution to the vanishing gradient problem. It creates an express lane for the learning signal to travel back through the network, ensuring that even the earliest layers get strong, clear updates. By adding the original input back at the end of the block, you're also reframing the learning task: the network now only needs to learn the *residual*, or the change from the input, which is often much easier.\n",
    "\n",
    "#### The Key Innovation: A Residual Block\n",
    "\n",
    "This block is the advanced building unit that gives your ResNet style model its power.\n",
    "\n",
    "* **`__init__`**: \n",
    "> This is the constructor where the block's components are defined.\n",
    ">    * Like the `PlainBlock`, it sets up two main **convolutional steps**.\n",
    ">    * Unlike the `PlainBlock`, it includes a `downsample` parameter. This is a special, optional layer whose only job is to resize the input on the **skip connection path** if its dimensions change, ensuring the two paths can be added together.\n",
    "\n",
    "* **`_initial_forward`**: \n",
    "> This helper method defines the **main transformation path**.\n",
    ">    * This path is functionally identical to the *entire* `forward` pass of the `PlainBlock`. Think of this as the **\"scenic route\"** where the detailed feature extraction happens.\n",
    "\n",
    "* **`forward`**: \n",
    "> This is the main event where the block's signature logic happens. The operations here are the **exact code additions that create the skip connection** and are completely absent in the `PlainBlock`:\n",
    ">    1.  **Save the Input (`identity = x`)**: The very first step is to create a copy of the original, untouched input. This establishes the \"express lane\" or shortcut path. \n",
    ">    2.  **Transform the Input (`out = _initial_forward(x)`)**: The input is also sent down the \"scenic route\" for transformation.\n",
    ">    3.  **Merge the Paths (`out += identity`)**: This is the pivotal moment. The transformed output (`out`) is merged with the original `identity` via addition. \n",
    ">        * This step, adding the original input back, is the defining operation of a residual connection.\n",
    ">    4.  **Final Activation (`F.relu(out`))**: The activation function is applied only *after* the two paths have been combined."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40038687",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "class ResidualBlock(nn.Module):\n",
    "    \"\"\"\n",
    "    A fundamental building block for ResNet architectures.\n",
    "\n",
    "    This block implements a residual connection, allowing the network to learn\n",
    "    an identity function if needed. It consists of a main path with two\n",
    "    convolutional layers and a \"skip connection\" that adds the input of the\n",
    "    block to its output. This helps mitigate the vanishing gradient problem in\n",
    "    very deep networks.\n",
    "\n",
    "    Args:\n",
    "        in_channels (int): The number of channels in the input tensor.\n",
    "        out_channels (int): The number of channels produced by the convolutions.\n",
    "        stride (int, optional): The stride for the first convolutional layer,\n",
    "                                used for downsampling. Defaults to 1.\n",
    "        downsample (nn.Module, optional): A module to downsample the input\n",
    "                                          (identity) so its dimensions match the\n",
    "                                          output for the skip connection.\n",
    "                                          Defaults to None.\n",
    "    \"\"\"\n",
    "    def __init__(self, in_channels, out_channels, stride=1, downsample=None):\n",
    "        # Initialize the parent nn.Module.\n",
    "        super(ResidualBlock, self).__init__()\n",
    "\n",
    "        # First component of the main path: Conv -> BatchNorm -> ReLU.\n",
    "        self.conv_block_1 = nn.Sequential(\n",
    "            nn.Conv2d(\n",
    "                in_channels,\n",
    "                out_channels,\n",
    "                kernel_size=3,\n",
    "                stride=stride,\n",
    "                padding=1,\n",
    "                bias=False,\n",
    "            ),\n",
    "            nn.BatchNorm2d(out_channels),\n",
    "            nn.ReLU(inplace=True),\n",
    "        )\n",
    "\n",
    "        # Second component of the main path: Conv -> BatchNorm.\n",
    "        # Note: The final ReLU is applied after the skip connection.\n",
    "        self.conv_block_2 = nn.Sequential(\n",
    "            nn.Conv2d(\n",
    "                out_channels,\n",
    "                out_channels,\n",
    "                kernel_size=3,\n",
    "                stride=1,\n",
    "                padding=1,\n",
    "                bias=False,\n",
    "            ),\n",
    "            nn.BatchNorm2d(out_channels),\n",
    "        )\n",
    "\n",
    "        # Optional downsampling layer for the skip connection.\n",
    "        self.downsample = downsample\n",
    "\n",
    "    def _initial_forward(self, x):\n",
    "        \"\"\"Defines the main convolutional path of the block.\"\"\"\n",
    "        out = self.conv_block_1(x)\n",
    "        out = self.conv_block_2(out)\n",
    "        return out\n",
    "\n",
    "    def forward(self, x):\n",
    "        \"\"\"\n",
    "        Defines the forward pass, combining the main path and the skip connection.\n",
    "\n",
    "        Args:\n",
    "            x (torch.Tensor): The input tensor.\n",
    "\n",
    "        Returns:\n",
    "            torch.Tensor: The output tensor of the residual block.\n",
    "        \"\"\"\n",
    "        # Store the input for the skip connection.\n",
    "        identity = x\n",
    "\n",
    "        # Pass the input through the main convolutional path.\n",
    "        out = self._initial_forward(x)\n",
    "\n",
    "        # If needed, apply downsampling to the identity to match dimensions.\n",
    "        if self.downsample is not None:\n",
    "            identity = self.downsample(x)\n",
    "\n",
    "        # Add the original input (identity) to the output of the main path.\n",
    "        out += identity\n",
    "\n",
    "        # Apply the final activation function.\n",
    "        out = F.relu(out)\n",
    "\n",
    "        # Return the final output of the block.\n",
    "        return out"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "628ef958",
   "metadata": {},
   "source": [
    "#### Assembling the `SimpleResNet` Architecture\n",
    "\n",
    "The `SimpleResNet` class assembles the `ResidualBlocks` into a full network. While its blueprint is nearly identical to the `PlainCNN` to ensure a fair comparison, the internal mechanics are significantly different and more powerful.\n",
    "\n",
    "* **`__init__`**: \n",
    "> This method lays out the network's blueprint.  \n",
    ">    * Its structure is **intentionally identical** to the `PlainCNN`'s constructor to ensure a fair comparison.\n",
    ">    * It begins the process of **setting up the skip connection** by calling the `_make_residual_block` helper.\n",
    "\n",
    "* **`_get_initial_block` & `_get_final_block`**: \n",
    "> These helpers create the network's standard entry and exit points (the \"stem\" and \"head\").  \n",
    ">    * The layers they produce are **100% identical** to the start and end blocks of the `PlainCNN`.\n",
    ">    * They are standard components and play **no role** in the skip connection logic.\n",
    "\n",
    "* **`_make_residual_block`**: \n",
    "> This is the advanced counterpart to the `PlainCNN`'s simple `_make_layer` method, and its main job is to **configure the skip connection** for an entire stage.\n",
    ">    * It intelligently checks if the image dimensions or channels are changing.\n",
    ">    * If so, it creates a `downsample` layer and passes it to the *first* `ResidualBlock` of that stage.\n",
    ">    * This configuration logic is completely absent in the `PlainCNN`.\n",
    "\n",
    "* **`forward`**: \n",
    "> Defines the end-to-end data flow through the network.\n",
    ">    * A key difference from `PlainCNN.forward` is that this function **lacks explicit `F.relu` calls** between stages.\n",
    ">    * This is because the skip connection is **executed** within each `ResidualBlock` that is called from here. The actual `out += identity` merge and subsequent `ReLU` happen *inside* the blocks themselves."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "597d332e",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "class SimpleResNet(nn.Module):\n",
    "    \"\"\"\n",
    "    A simplified ResNet-style architecture for image classification.\n",
    "\n",
    "    This network is built by stacking ResidualBlock modules. It consists of an\n",
    "    initial convolutional layer, followed by three main stages of residual\n",
    "    blocks, and a final classification head with global average pooling.\n",
    "\n",
    "    Args:\n",
    "        num_classes (int, optional): The number of output classes for the final\n",
    "                                     classification layer. Defaults to 5.\n",
    "        num_blocks (list of int, optional): A list defining the number of\n",
    "                                            ResidualBlocks in each of the three\n",
    "                                            main stages. Defaults to [2, 2, 2].\n",
    "    \"\"\"\n",
    "    def __init__(self, num_classes=5, num_blocks=[2, 2, 2]):\n",
    "        # Initialize the parent nn.Module.\n",
    "        super(SimpleResNet, self).__init__()\n",
    "\n",
    "        # Store the number of classes for the final layer.\n",
    "        self.num_classes = num_classes\n",
    "        \n",
    "        # Initialize the number of input channels for the first residual stage.\n",
    "        self.in_channels = 32\n",
    "\n",
    "        # Define the initial convolutional layer.\n",
    "        self.initial_block = self._get_initial_block()\n",
    "\n",
    "        # Construct the main stages of the network using residual blocks.\n",
    "        self.res_block1 = self._make_residual_block(32, num_blocks[0], stride=1)\n",
    "        self.res_block2 = self._make_residual_block(64, num_blocks[1], stride=2)\n",
    "        self.res_block3 = self._make_residual_block(128, num_blocks[2], stride=2)\n",
    "\n",
    "        # Define the final classification head.\n",
    "        self.final_block = self._get_final_block()\n",
    "\n",
    "    def _make_residual_block(self, out_channels, num_blocks, stride):\n",
    "        \"\"\"\n",
    "        Builds a residual stage by stacking multiple ResidualBlocks.\n",
    "\n",
    "        Args:\n",
    "            out_channels (int): The number of output channels for the blocks.\n",
    "            num_blocks (int): The number of blocks to stack in this stage.\n",
    "            stride (int): The stride for the first block, used for downsampling.\n",
    "\n",
    "        Returns:\n",
    "            nn.Sequential: A sequential container of the stacked blocks.\n",
    "        \"\"\"\n",
    "        # Initialize the downsample layer as None.\n",
    "        downsample = None\n",
    "        \n",
    "        # Define the downsample layer if stride is not 1 or if channel dimensions change.\n",
    "        if stride != 1 or self.in_channels != out_channels:\n",
    "            downsample = nn.Sequential(\n",
    "                nn.Conv2d(\n",
    "                    self.in_channels,\n",
    "                    out_channels,\n",
    "                    kernel_size=1,\n",
    "                    stride=stride,\n",
    "                    bias=False,\n",
    "                ),\n",
    "                nn.BatchNorm2d(out_channels),\n",
    "            )\n",
    "\n",
    "        # Initialize a list to hold the layers for this stage.\n",
    "        layers = []\n",
    "        \n",
    "        # The first block in a stage handles downsampling and channel changes.\n",
    "        first_block = ResidualBlock(self.in_channels, out_channels, stride, downsample)\n",
    "        layers.append(first_block)\n",
    "        \n",
    "        # Update the number of input channels for the subsequent stage.\n",
    "        self.in_channels = out_channels\n",
    "\n",
    "        # Add the remaining residual blocks for this stage.\n",
    "        for _ in range(1, num_blocks):\n",
    "            layers.append(ResidualBlock(out_channels, out_channels))\n",
    "\n",
    "        # Return the blocks wrapped in a sequential container.\n",
    "        return nn.Sequential(*layers)\n",
    "\n",
    "    def _get_initial_block(self):\n",
    "        \"\"\"Constructs the initial convolutional layer of the network.\"\"\"\n",
    "        initial_block = nn.Sequential(\n",
    "            nn.Conv2d(3, 32, kernel_size=3, stride=1, padding=1, bias=False),\n",
    "            nn.BatchNorm2d(32),\n",
    "            nn.ReLU(inplace=True),\n",
    "        )\n",
    "        return initial_block\n",
    "\n",
    "    def _get_final_block(self):\n",
    "        \"\"\"Constructs the final classification head of the network.\"\"\"\n",
    "        final_block = nn.Sequential(\n",
    "            nn.AdaptiveAvgPool2d((1, 1)),\n",
    "            nn.Flatten(),\n",
    "            nn.Linear(128, self.num_classes),\n",
    "        )\n",
    "        return final_block\n",
    "\n",
    "    def forward(self, x):\n",
    "        \"\"\"\n",
    "        Defines the forward pass for the SimpleResNet.\n",
    "\n",
    "        Args:\n",
    "            x (torch.Tensor): The input image tensor.\n",
    "\n",
    "        Returns:\n",
    "            torch.Tensor: The output logits from the final classification layer.\n",
    "        \"\"\"\n",
    "        # Pass input through the initial convolutional block.\n",
    "        out = self.initial_block(x)\n",
    "        \n",
    "        # Pass through the three main stages of residual blocks.\n",
    "        out = self.res_block1(out)\n",
    "        out = self.res_block2(out)\n",
    "        out = self.res_block3(out)\n",
    "        \n",
    "        # Pass through the final classification head.\n",
    "        out = self.final_block(out)\n",
    "        \n",
    "        # Return the final output logits.\n",
    "        return out"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b4582a73",
   "metadata": {},
   "source": [
    "### Initialize the Models\n",
    "\n",
    "With your architectural blueprints complete, it's time to construct your two competitors.\n",
    "\n",
    "* Create instances of both, the `SimpleResNet` and the `PlainCNN` models, preparing them for the training process.\n",
    "    * `PlainCNN` will serve as the baseline control in your experiment. You'll use its performance as a benchmark to see just how much of an improvement the skip connections in `SimpleResNet` really provide."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d4461772-53ad-4913-88e6-4e64b7806749",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Set the random seed to ensure that both models are always initialized with the same random weights.\n",
    "torch.manual_seed(SEED)\n",
    "\n",
    "# Create an instance of the SimpleResNet model.\n",
    "resnet_model = SimpleResNet(num_classes=num_classes)\n",
    "\n",
    "# Create an instance of the PlainCNN model for your baseline comparison.\n",
    "plain_model = PlainCNN(num_classes=num_classes)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c281f81-e939-41d3-8509-14e41fe33df2",
   "metadata": {},
   "source": [
    "### Examining the Model Architecture\n",
    "\n",
    "Before you start the resource-intensive process of training, a professional workflow includes a critical quality check: **examining the model's architecture**. Think of this as an architect reviewing the final blueprints before construction begins. It allows you to verify that every component is connected correctly and to understand the true complexity of what you've built.\n",
    "\n",
    "In a real-world project, this step is essential. It helps you catch errors early, estimate computational costs, and plan for deployment. You'll use the [torchinfo](https://github.com/TylerYep/torchinfo) library to generate this blueprint, which provides a layer-by-layer breakdown with vital details:\n",
    "\n",
    "* **Output Shape Flow**: You can trace the journey of your data, seeing how its dimensions change at each layer. This is invaluable for debugging common dimension mismatch errors before you even start training.\n",
    "\n",
    "* **Parameter Count**: The summary reveals the number of trainable parameters in each layer and in total. This number directly impacts your choice of hardware, your training time, and the model's final deployment size. A model with 10 million parameters has very different requirements than one with 100 million.\n",
    "\n",
    "This step ensures your model is built as intended and gives you a clear picture of its computational profile before you commit to training.\n",
    "\n",
    "* Call the `torchinfo.summary` function, the main command that inspects your model and generates the architectural summary based on these arguments:\n",
    "    * `model=resnet_model`: The PyTorch model you want to analyze.\n",
    "    * `input_size=...`: This specifies the **shape of a sample input tensor** (batch size, channels, height, width).\n",
    "        * `torchinfo` performs a \"dry run\" with this shape to calculate the output size of each layer.\n",
    "    * `col_names=...`: This lets you customize **which columns** of information to display in the summary.\n",
    "    *  `depth=...`: This controls **how many levels deep** the summary should go into your nested modules (like `ResidualBlock`).\n",
    "* The `display_torch_summary` helper function takes this summary object and prints it as a clean, easy to read table."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30fb7e99-12a1-402c-b3d1-f742d0a6e8d8",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Define a configuration dictionary to store parameters for the model summary.\n",
    "config = {\n",
    "    \"input_size\": (batch_size, 3, 64, 64),\n",
    "    \"attr_names\": [\"input_size\", \"output_size\", \"num_params\", \"trainable\"],\n",
    "    \"col_names_display\": [\"Input Shape \", \"Output Shape\", \"Param #\", \"Trainable\"],\n",
    "    \"depth\": 3\n",
    "}\n",
    "\n",
    "# Generate the model summary object using torchinfo with the specified configuration.\n",
    "summary = torchinfo.summary(\n",
    "    model=resnet_model, \n",
    "    input_size=config[\"input_size\"], \n",
    "    col_names=config[\"attr_names\"], \n",
    "    depth=config[\"depth\"]\n",
    ")\n",
    "\n",
    "# Display the summary as a styled HTML table.\n",
    "print(\"--- Model Summary ---\\n\")\n",
    "helper_utils.display_torch_summary(summary, config[\"attr_names\"], config[\"col_names_display\"], config[\"depth\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea329437-ad75-45b0-9b56-28698c38c102",
   "metadata": {},
   "source": [
    "<br>\n",
    "\n",
    "This summary, with a `depth=3`, is the proof that your architecture is correctly wired. By tracing the **Input Shape** and **Output Shape** columns, you can confirm your design logic is working exactly as intended.\n",
    "\n",
    "A key verification point is observing when the `downsample` layer appears within the first `ResidualBlock` of each stage:\n",
    "\n",
    "* **Stage 1 (`Sequential: 1-2`)**: Notice the input and output shapes are identical (`[...32, 64, 64]`). As expected, there's no `downsample` layer because neither the dimensions nor the channels change. The skip connection can be a direct identity copy.\n",
    "\n",
    "* **Stages 2 & 3 (`Sequential: 1-3` & `1-4`)**: Here you see the shapes change dramatically (e.g., from `[...32, 64, 64]` to `[...64, 32, 32]`). This confirms that the `downsample` layer has been correctly added inside the first `ResidualBlock` of these stages.\n",
    "\n",
    "This happens for two reasons: the number of **channels doubles**, and the **spatial dimensions are halved** (due to `stride=2`). The summary confirms that your `_make_residual_block` method correctly identified this change and added the `downsample` layer to make the skip connection possible. Your blueprint has been built to spec!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac4ce69a-a568-4873-8be6-861725f8478a",
   "metadata": {},
   "source": [
    "## Launching the Experiment: The Training Phase\n",
    "Your two models, the `PlainCNN` and the `SimpleResNet`, are now fully constructed and waiting at the starting line. Before the race can begin, you need to define two final components for the training process.\n",
    "\n",
    "### Loss Function and Optimizers\n",
    "\n",
    "* Define the loss function and optimizers that will drive the training process.\n",
    "    * You'll use the same loss function for both models, but you will create a separate optimizer for each one. This is necessary to track and update the weights for the `PlainCNN` and `SimpleResNet` models independently."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a75c7b9d-8197-4e61-b74c-57ca1de24ac8",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Use CrossEntropyLoss, a standard loss function for multi-class classification tasks.\n",
    "loss_function = nn.CrossEntropyLoss()\n",
    "\n",
    "# Create an Adam optimizer to update the weights of the ResNet model.\n",
    "optimizer_resnet = optim.Adam(resnet_model.parameters(), lr=0.001)\n",
    "\n",
    "# Create a separate Adam optimizer for the Plain CNN model to train it independently.\n",
    "optimizer_plain = optim.Adam(plain_model.parameters(), lr=0.001)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b0ee0cc7",
   "metadata": {},
   "source": [
    "### Training and Validation\n",
    "\n",
    "You will now train both of your models using the provided `training_loop_16_mixed` training function. This custom loop handles the complete training and validation cycle, and it uses a **16 bit mixed precision strategy** to accelerate performance. This optimization is implemented using **PyTorch's own built in libraries**.\n",
    "\n",
    "To judge the performance of the models fairly, the function will track and return the following key results for later comparison:\n",
    "\n",
    "* `history`: A Python dictionary that records the performance after each epoch. It contains the **training loss**, **validation loss**, and **validation accuracy**, which are essential for plotting the learning curves to visualize the training progress.\n",
    "\n",
    "* `final_cm`: A confusion matrix calculated on the validation data after the final epoch. This is a powerful tool that shows exactly which classes the model is confusing with others, helping you analyze its specific strengths and weaknesses."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "446b4001-ab3d-4a98-9bbb-3575df215458",
   "metadata": {},
   "source": [
    "* Feel free to set a different value for `num_epochs`. It's currently set to **10**, which is a good starting point and sufficient to get a meaningful performance comparison for these experiments."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1fdff53d-cc75-4059-8e76-89df254b8075",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Define the total number of full training cycles (epochs) to run.\n",
    "num_epochs = 10"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e37853e-4cc5-4859-aa1c-2f6ab00c4312",
   "metadata": {},
   "source": [
    "* First up is your baseline model. Send the `PlainCNN` through the training loop to see how it performs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ff1e539-32a0-4010-b6df-0d41733583a2",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Training of `plain_model`\n",
    "trained_plain, history_plain, cm_plain = helper_utils.training_loop_16_mixed(\n",
    "    model=plain_model,\n",
    "    train_loader=train_loader,\n",
    "    val_loader=val_loader,\n",
    "    loss_function=loss_function,\n",
    "    optimizer=optimizer_plain,\n",
    "    num_epochs=num_epochs,\n",
    "    device=device\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c7b7b6f5-89fd-48fc-9746-c58ea5b87729",
   "metadata": {},
   "source": [
    "* With the baseline set, it's time for your advanced `SimpleResNet`. Run it through the same training process and see if the skip connections give it an edge."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46296c61-0267-4f2c-bc12-4303f90600e7",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Training of `resnet_model`\n",
    "trained_resnet, history_resnet, cm_resnet = helper_utils.training_loop_16_mixed(\n",
    "    model=resnet_model,\n",
    "    train_loader=train_loader,\n",
    "    val_loader=val_loader,\n",
    "    loss_function=loss_function,\n",
    "    optimizer=optimizer_resnet,\n",
    "    num_epochs=num_epochs,\n",
    "    device=device\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10f0a42e-c3dd-4ced-a636-84debeca1ad8",
   "metadata": {},
   "source": [
    "### The Tale of the Tape: Visualizing Performance\n",
    "\n",
    "With both models trained, it's time to see which one performed better. You'll now plot the learning curves from both history objects of the trained models.\n",
    "\n",
    "This will give you a direct, visual comparison of how the `PlainCNN` (your baseline) and the `SimpleResNet` (your advanced model) performed on key metrics like **loss** and **accuracy** throughout the training process."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca5b924c-7a1e-4040-853d-95162dca09d2",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Plot the training metrics from both models for a direct visual comparison.\n",
    "helper_utils.plot_training_logs(history_plain, history_resnet)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c7dd8a77-3b71-4e2a-8b0d-8227de8650f6",
   "metadata": {},
   "source": [
    "<br>\n",
    "\n",
    "The plots clearly illustrate the powerful advantage of the ResNet style architecture. While exact numbers can vary slightly with different number of epochs, the trends you see here are a classic demonstration of why skip connections are so effective.\n",
    "\n",
    "* **Training and Validation Loss**: The `SimpleResNet` model (blue lines) consistently achieves a **lower and more stable loss** on both the training and validation sets. In contrast, the `PlainCNN` (red lines) struggles to keep its validation loss consistently decreasing, a common sign that a deep network is having a harder time optimizing and generalizing to new data.\n",
    "\n",
    "* **Validation Accuracy**: The accuracy graph tells an even clearer story. The `SimpleResNet` model not only reaches a **significantly higher final accuracy** (around 79% in this run), but its learning curve is also much smoother and more consistent. The `PlainCNN` improves, but its progress is more erratic and it plateaus at a lower performance level (around 67%).\n",
    "\n",
    "These results strongly indicate that the **residual connections** are the key difference maker. By providing a \"shortcut\" for the gradient to flow through the network, they help prevent the vanishing gradient problem. This allows the ResNet style model to learn more effectively from the data, leading to the more stable learning, lower loss, and higher final accuracy you see here."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b9e0f18-f594-4be9-9cec-b828adb6b449",
   "metadata": {},
   "source": [
    "## Visualizing Predictions and Confusion Matrices\n",
    "\n",
    "The learning curves gave you a high level view of the performance, but seeing your model's predictions on individual images is a great way to move beyond aggregate scores and get a true qualitative feel of it. This is where you can see **what** your model gets right and, more importantly, *where* it might be making mistakes. \n",
    "\n",
    "* First, you'll visualize a batch of predictions from your superior `SimpleResNet` model to see how it performs on different landscape types."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c55aeb8f-ccde-4df1-abea-02bf2cd4273c",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Get the list of class names (e.g., 'Forest', 'River') from the dataset for plotting.\n",
    "class_names = val_loader.dataset.classes\n",
    "\n",
    "# Visualize predictions from the SimpleResNet model on the validation data.\n",
    "helper_utils.visualize_predictions(trained_resnet, val_loader, class_names, device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e5c47fe9-0a8c-42b4-a132-3c788ec690c0",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# ### Uncomment and execute the line below if you wish to see the predictions by the trained PlainCNN model.\n",
    "\n",
    "# # Visualize predictions from the PlainCNN model on the validation data.\n",
    "# helper_utils.visualize_predictions(trained_plain, val_loader, class_names, device)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1335414c-1655-4170-8958-2fd0fc949ab6",
   "metadata": {},
   "source": [
    "<br>\n",
    "\n",
    "Visualizing individual predictions gives you a good gut feeling for the model's performance, but now it's time for a more detailed analysis. A single overall accuracy score can be misleading. To truly understand the impact of the skip connections, you need a more nuanced story that reveals where the ResNet style architecture excels. \n",
    "\n",
    "* First, generate per class accuracy and a confusion matrix for the trained `PlainCNN` model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ba7cf81-3ac4-4331-af1f-d5632313749d",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Plot the confusion matrix for the baseline PlainCNN to analyze its error patterns.\n",
    "helper_utils.plot_confusion_matrix(cm_plain, class_names)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "67106bbd-727b-4abe-8998-2198dace1b79",
   "metadata": {},
   "source": [
    "<br>\n",
    "\n",
    "* Now, generate per class accuracy and a confusion matrix for the trained `SimpleResNet` model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d62a5520-bf89-4299-8300-9a79b2182303",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Plot the confusion matrix for the ResNet style model to analyze its error patterns.\n",
    "helper_utils.plot_confusion_matrix(cm_resnet, class_names)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "31401312-d907-44b2-8fc5-6332d17e59ce",
   "metadata": {},
   "source": [
    "<br>\n",
    "\n",
    "**The Clear Advantage of Skip Connections**\n",
    "\n",
    "The most significant trend is the **dramatic performance increase** for the ResNet style model on classes defined by complex details, fine lines, or intricate patterns.\n",
    "\n",
    "* **Learning Fine Grained Features**: The baseline `PlainCNN` often struggles with categories like `Airport`, `Highway`, and `River`. This is likely because as the data passes through many sequential layers, the critical information about these fine linear features gets diluted, a classic symptom of a network struggling with information flow. The ResNet style model, however, excels on these classes. Its **skip connections** act as an \"information highway,\" preserving these crucial details and allowing the model to learn the complex patterns more effectively.\n",
    "\n",
    "* **Recognizing Dense Structures**: Similarly, for classes like `Residential` areas and `Parking` lots, the ResNet style model shows a major improvement. Its ability to maintain a strong gradient signal helps it distinguish between the densely packed, repeating objects in these images."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "862a0555",
   "metadata": {},
   "source": [
    "## Conclusion\n",
    "\n",
    "This experiment has clearly demonstrated the powerful advantage of a well-designed custom architecture. While both the `PlainCNN` and the `SimpleResNet` were built with the same number of layers, the `SimpleResNet` achieved significantly better performance, showcasing more stable training and higher final accuracy. This outcome is a direct result of its core architectural feature: the **residual connection**.\n",
    "\n",
    "The `PlainCNN`, with its purely sequential design, likely struggled with the vanishing gradient problem, where the learning signal faded as it propagated backward through the network's depth. In contrast, the `SimpleResNet`'s skip connections created a direct \"express lane\" for the gradient. By adding the original input back to the transformed output (`out += identity`), the network preserves the gradient signal, ensuring that even the earliest layers receive strong updates and continue to learn effectively.\n",
    "\n",
    "You've seen firsthand that building state-of-the-art models is about more than just stacking layers. It's about understanding the challenges that arise during training and engineering intelligent solutions directly into the model's architecture. By implementing the residual block, you've moved beyond a simple linear data flow and taken direct control, proving how a simple, elegant idea can solve a complex problem and unlock the potential of deep neural networks."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0rc1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
